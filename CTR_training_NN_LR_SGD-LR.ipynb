{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error, accuracy_score\n",
    "from sklearn import ensemble, linear_model, neural_network, naive_bayes\n",
    "from scipy.sparse import lil_matrix, csr_matrix, save_npz, load_npz\n",
    "import time\n",
    "import math\n",
    "import collections\n",
    "import itertools as it\n",
    "#import xgboost as xgb\n",
    "#from xgboost.sklearn import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2430981, 303925)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = load_npz('proper_datasets/X_train.npz')\n",
    "X_val = load_npz('proper_datasets/X_val.npz')\n",
    "\n",
    "y_train = np.loadtxt('proper_datasets/y_train.csv')\n",
    "y_val = np.loadtxt('proper_datasets/y_val.csv')\n",
    "\n",
    "X_test = load_npz('proper_datasets/X_test.npz')\n",
    "\n",
    "len(y_train), len(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logr 0 {'C': 0.0001, 'solver': 'saga'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6957531538557002 0.025771153405458685\n",
      "Time: 319.99630\n",
      "logr 1 {'C': 0.0001, 'solver': 'sag'}\n",
      "0.709588356678439 0.025771141086961834\n",
      "Time: 53.79408\n",
      "logr 2 {'C': 0.001, 'solver': 'saga'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.768939906258383 0.02576332480652155\n",
      "Time: 286.59739\n",
      "logr 3 {'C': 0.001, 'solver': 'sag'}\n",
      "0.7780029291932661 0.025764142586747637\n",
      "Time: 112.74945\n",
      "logr 4 {'C': 0.01, 'solver': 'saga'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8141000953741624 0.025674517285461336\n",
      "Time: 279.99101\n",
      "logr 5 {'C': 0.01, 'solver': 'sag'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8149908725130373 0.025679895188072777\n",
      "Time: 252.17742\n",
      "logr 6 {'C': 0.1, 'solver': 'saga'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8050420111498807 0.02560388170711711\n",
      "Time: 279.80700\n",
      "logr 7 {'C': 0.1, 'solver': 'sag'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8052286715914903 0.02560516187267451\n",
      "Time: 254.11153\n",
      "logr 8 {'C': 1, 'solver': 'saga'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7991480267830026 0.0256393904464077\n",
      "Time: 279.55999\n",
      "logr 9 {'C': 1, 'solver': 'sag'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7991447180098933 0.025639453174476846\n",
      "Time: 246.42910\n",
      "logr 10 {'C': 10, 'solver': 'saga'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7953781280578646 0.025674036116218723\n",
      "Time: 275.83978\n",
      "logr 11 {'C': 10, 'solver': 'sag'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.794584120307903 0.025675039329844163\n",
      "Time: 247.41715\n",
      "logr 12 {'C': 100, 'solver': 'saga'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7944665040184642 0.025680239826943656\n",
      "Time: 272.23257\n",
      "logr 13 {'C': 100, 'solver': 'sag'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7931863022139474 0.025681976382137785\n",
      "Time: 247.65216\n",
      "logr 14 {'C': 10000000000.0, 'solver': 'saga'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7943407543409393 0.025680957279657624\n",
      "Time: 265.28217\n",
      "logr 15 {'C': 10000000000.0, 'solver': 'sag'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7930380430996546 0.025682616116769183\n",
      "Time: 252.31843\n",
      "sgd 0 {'alpha': 0.0001, 'l1_ratio': 0, 'loss': 'log', 'penalty': 'elasticnet', 'shuffle': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7826516413160858 0.02572416397351493\n",
      "Time: 9.56355\n",
      "sgd 1 {'alpha': 0.0001, 'l1_ratio': 0, 'loss': 'log', 'penalty': 'elasticnet', 'shuffle': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7816283013609684 0.02572578669136769\n",
      "Time: 6.09935\n",
      "sgd 2 {'alpha': 0.0001, 'l1_ratio': 0.5, 'loss': 'log', 'penalty': 'elasticnet', 'shuffle': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.759234793897501 0.02573532629703822\n",
      "Time: 9.94557\n",
      "sgd 3 {'alpha': 0.0001, 'l1_ratio': 0.5, 'loss': 'log', 'penalty': 'elasticnet', 'shuffle': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7584678675589727 0.02573564254575731\n",
      "Time: 6.83439\n",
      "sgd 4 {'alpha': 0.0001, 'l1_ratio': 1, 'loss': 'log', 'penalty': 'elasticnet', 'shuffle': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7349877704160022 0.025714421425708414\n",
      "Time: 9.50154\n",
      "sgd 5 {'alpha': 0.0001, 'l1_ratio': 1, 'loss': 'log', 'penalty': 'elasticnet', 'shuffle': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6988804660239041 0.025753174176889708\n",
      "Time: 6.30336\n",
      "sgd 6 {'alpha': 0.001, 'l1_ratio': 0, 'loss': 'log', 'penalty': 'elasticnet', 'shuffle': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6618766634123335 0.02592810431920113\n",
      "Time: 9.11952\n",
      "sgd 7 {'alpha': 0.001, 'l1_ratio': 0, 'loss': 'log', 'penalty': 'elasticnet', 'shuffle': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6618280749756902 0.025928372441780433\n",
      "Time: 6.00534\n",
      "sgd 8 {'alpha': 0.001, 'l1_ratio': 0.5, 'loss': 'log', 'penalty': 'elasticnet', 'shuffle': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6608904045351642 0.026322838491512034\n",
      "Time: 9.96257\n",
      "sgd 9 {'alpha': 0.001, 'l1_ratio': 0.5, 'loss': 'log', 'penalty': 'elasticnet', 'shuffle': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.660018355378075 0.026323308659649228\n",
      "Time: 6.85239\n",
      "sgd 10 {'alpha': 0.001, 'l1_ratio': 1, 'loss': 'log', 'penalty': 'elasticnet', 'shuffle': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6304863492246044 0.026366173536140892\n",
      "Time: 9.29553\n",
      "sgd 11 {'alpha': 0.001, 'l1_ratio': 1, 'loss': 'log', 'penalty': 'elasticnet', 'shuffle': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6284834820341606 0.026386319443092825\n",
      "Time: 6.18835\n",
      "sgd 12 {'alpha': 0.01, 'l1_ratio': 0, 'loss': 'log', 'penalty': 'elasticnet', 'shuffle': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6296261660124587 0.031272623874832435\n",
      "Time: 8.99751\n",
      "sgd 13 {'alpha': 0.01, 'l1_ratio': 0, 'loss': 'log', 'penalty': 'elasticnet', 'shuffle': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6296034772825668 0.031273248026564796\n",
      "Time: 5.91134\n",
      "sgd 14 {'alpha': 0.01, 'l1_ratio': 0.5, 'loss': 'log', 'penalty': 'elasticnet', 'shuffle': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.634639324008852 0.04201053864935181\n",
      "Time: 9.80856\n",
      "sgd 15 {'alpha': 0.01, 'l1_ratio': 0.5, 'loss': 'log', 'penalty': 'elasticnet', 'shuffle': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6348135317280209 0.04200087419274423\n",
      "Time: 6.60238\n",
      "sgd 16 {'alpha': 0.01, 'l1_ratio': 1, 'loss': 'log', 'penalty': 'elasticnet', 'shuffle': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.614287256206582 0.0499449647592608\n",
      "Time: 9.28853\n",
      "sgd 17 {'alpha': 0.01, 'l1_ratio': 1, 'loss': 'log', 'penalty': 'elasticnet', 'shuffle': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6137340635714089 0.04994772113704579\n",
      "Time: 6.15935\n",
      "sgd 18 {'alpha': 0.1, 'l1_ratio': 0, 'loss': 'log', 'penalty': 'elasticnet', 'shuffle': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6172113396185679 0.08705135995169357\n",
      "Time: 9.04052\n",
      "sgd 19 {'alpha': 0.1, 'l1_ratio': 0, 'loss': 'log', 'penalty': 'elasticnet', 'shuffle': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6172015925923644 0.08701534167680218\n",
      "Time: 5.94534\n",
      "sgd 20 {'alpha': 0.1, 'l1_ratio': 0.5, 'loss': 'log', 'penalty': 'elasticnet', 'shuffle': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5973883658256482 0.182903561437475\n",
      "Time: 9.69655\n",
      "sgd 21 {'alpha': 0.1, 'l1_ratio': 0.5, 'loss': 'log', 'penalty': 'elasticnet', 'shuffle': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5981630995647642 0.18275902459531124\n",
      "Time: 6.59538\n",
      "sgd 22 {'alpha': 0.1, 'l1_ratio': 1, 'loss': 'log', 'penalty': 'elasticnet', 'shuffle': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5778615679744405 0.2336376806605809\n",
      "Time: 9.19653\n",
      "sgd 23 {'alpha': 0.1, 'l1_ratio': 1, 'loss': 'log', 'penalty': 'elasticnet', 'shuffle': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.578443912041662 0.23341802942052245\n",
      "Time: 6.14435\n",
      "sgd 24 {'alpha': 1, 'l1_ratio': 0, 'loss': 'log', 'penalty': 'elasticnet', 'shuffle': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6086477295313021 0.2702490420673\n",
      "Time: 9.01552\n",
      "sgd 25 {'alpha': 1, 'l1_ratio': 0, 'loss': 'log', 'penalty': 'elasticnet', 'shuffle': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6086457410075615 0.27009016625678806\n",
      "Time: 5.92234\n",
      "sgd 26 {'alpha': 1, 'l1_ratio': 0.5, 'loss': 'log', 'penalty': 'elasticnet', 'shuffle': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 0.47954839617077405\n",
      "Time: 9.01452\n",
      "sgd 27 {'alpha': 1, 'l1_ratio': 0.5, 'loss': 'log', 'penalty': 'elasticnet', 'shuffle': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 0.479380914173123\n",
      "Time: 5.85934\n",
      "sgd 28 {'alpha': 1, 'l1_ratio': 1, 'loss': 'log', 'penalty': 'elasticnet', 'shuffle': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 0.47938456818867997\n",
      "Time: 8.78650\n",
      "sgd 29 {'alpha': 1, 'l1_ratio': 1, 'loss': 'log', 'penalty': 'elasticnet', 'shuffle': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 0.479380914173123\n",
      "Time: 5.74233\n",
      "sgd 30 {'alpha': 10, 'l1_ratio': 0, 'loss': 'log', 'penalty': 'elasticnet', 'shuffle': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6053600641126133 0.45354467869442805\n",
      "Time: 9.06052\n",
      "sgd 31 {'alpha': 10, 'l1_ratio': 0, 'loss': 'log', 'penalty': 'elasticnet', 'shuffle': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6053600315138634 0.4534866230075153\n",
      "Time: 5.92634\n",
      "sgd 32 {'alpha': 10, 'l1_ratio': 0.5, 'loss': 'log', 'penalty': 'elasticnet', 'shuffle': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 0.49723094107982313\n",
      "Time: 8.99951\n",
      "sgd 33 {'alpha': 10, 'l1_ratio': 0.5, 'loss': 'log', 'penalty': 'elasticnet', 'shuffle': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 0.497230858503193\n",
      "Time: 5.91134\n",
      "sgd 34 {'alpha': 10, 'l1_ratio': 1, 'loss': 'log', 'penalty': 'elasticnet', 'shuffle': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 0.49723205260138653\n",
      "Time: 8.92651\n",
      "sgd 35 {'alpha': 10, 'l1_ratio': 1, 'loss': 'log', 'penalty': 'elasticnet', 'shuffle': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 0.497230858503193\n",
      "Time: 5.71733\n",
      "sgd 36 {'alpha': 100, 'l1_ratio': 0, 'loss': 'log', 'penalty': 'elasticnet', 'shuffle': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6048639437387303 0.4945388939923561\n",
      "Time: 9.05452\n",
      "sgd 37 {'alpha': 100, 'l1_ratio': 0, 'loss': 'log', 'penalty': 'elasticnet', 'shuffle': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6048639111399805 0.49453449997142374\n",
      "Time: 5.89834\n",
      "sgd 38 {'alpha': 100, 'l1_ratio': 0.5, 'loss': 'log', 'penalty': 'elasticnet', 'shuffle': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 0.49939540149146106\n",
      "Time: 9.02252\n",
      "sgd 39 {'alpha': 100, 'l1_ratio': 0.5, 'loss': 'log', 'penalty': 'elasticnet', 'shuffle': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 0.4993954050477959\n",
      "Time: 5.90934\n",
      "sgd 40 {'alpha': 100, 'l1_ratio': 1, 'loss': 'log', 'penalty': 'elasticnet', 'shuffle': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 0.4993954197243582\n",
      "Time: 8.81650\n",
      "sgd 41 {'alpha': 100, 'l1_ratio': 1, 'loss': 'log', 'penalty': 'elasticnet', 'shuffle': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 0.4993954050477959\n",
      "Time: 5.69633\n",
      "sgd 42 {'alpha': 10000000000.0, 'l1_ratio': 0, 'loss': 'log', 'penalty': 'elasticnet', 'shuffle': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6048078820386854 0.4999960523560448\n",
      "Time: 8.97951\n",
      "sgd 43 {'alpha': 10000000000.0, 'l1_ratio': 0, 'loss': 'log', 'penalty': 'elasticnet', 'shuffle': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6048077434939986 0.49999605235604355\n",
      "Time: 5.93634\n",
      "sgd 44 {'alpha': 10000000000.0, 'l1_ratio': 0.5, 'loss': 'log', 'penalty': 'elasticnet', 'shuffle': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 0.4999960524052837\n",
      "Time: 8.98851\n",
      "sgd 45 {'alpha': 10000000000.0, 'l1_ratio': 0.5, 'loss': 'log', 'penalty': 'elasticnet', 'shuffle': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 0.499996052405284\n",
      "Time: 5.88834\n",
      "sgd 46 {'alpha': 10000000000.0, 'l1_ratio': 1, 'loss': 'log', 'penalty': 'elasticnet', 'shuffle': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 0.49999605240528355\n",
      "Time: 8.86251\n",
      "sgd 47 {'alpha': 10000000000.0, 'l1_ratio': 1, 'loss': 'log', 'penalty': 'elasticnet', 'shuffle': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 0.499996052405284\n",
      "Time: 5.73633\n"
     ]
    }
   ],
   "source": [
    "def build_combinations(param_grid):\n",
    "    \n",
    "    '''\n",
    "    Create all possible hyperparameter combinations for a given model \n",
    "    '''\n",
    "  \n",
    "    combinations = []\n",
    "\n",
    "    allNames = sorted(param_grid)\n",
    "    tmp_combs = list(it.product(*(param_grid[Name] for Name in allNames)))\n",
    "    \n",
    "    for combi in tmp_combs:\n",
    "        my_dict = {}\n",
    "        my_dict[allNames[0]] = combi[0]\n",
    "    \n",
    "        for i in range(1,len(allNames)):\n",
    "            my_dict[allNames[i]] = combi[i]\n",
    "        \n",
    "        combinations.append(my_dict)\n",
    "        \n",
    "    return combinations\n",
    "\n",
    "clf_models = collections.OrderedDict({#'knn': neighbors.KNeighborsClassifier(),\n",
    "                                     #'xgb1': XGBClassifier(\n",
    "                                        #learning_rate =0.1,\n",
    "                                        #n_estimators=100,\n",
    "                                        #max_depth=5,\n",
    "                                        #min_child_weight=1,\n",
    "                                        #gamma=0,\n",
    "                                        #subsample=0.8,\n",
    "                                        #colsample_bytree=0.8,\n",
    "                                        #objective= 'binary:logistic',\n",
    "                                        #nthread=4,\n",
    "                                        #scale_pos_weight=1,\n",
    "                                        #seed=1234,\n",
    "                                        #random_state=1234),\n",
    "                                     #'xgb2': XGBClassifier(\n",
    "                                        #learning_rate =0.1,\n",
    "                                        #n_estimators=100,\n",
    "                                        #max_depth=5,\n",
    "                                        #min_child_weight=1,\n",
    "                                        #gamma=0,\n",
    "                                        #subsample=0.8,\n",
    "                                        #colsample_bytree=0.8,\n",
    "                                        #objective= 'binary:logistic',\n",
    "                                        #nthread=4,\n",
    "                                        #scale_pos_weight=1,\n",
    "                                        #seed=1234,\n",
    "                                        #random_state=1234),  \n",
    "                                     #'xgb3': XGBClassifier(\n",
    "                                        #learning_rate =0.1,\n",
    "                                     #   n_estimators=50,\n",
    "                                        #max_depth=5,\n",
    "                                        #min_child_weight=1,\n",
    "                                        #gamma=0,\n",
    "                                        #subsample=0.8,\n",
    "                                        #colsample_bytree=0.8,\n",
    "                                        #objective= 'binary:logistic',\n",
    "                                        #nthread=4,\n",
    "                                        #scale_pos_weight=1,\n",
    "                                     #   seed=1234,\n",
    "                                     #   random_state=1234),   \n",
    "                                     #'xgb4': XGBClassifier(\n",
    "                                     #   #learning_rate =0.1,\n",
    "                                     #   n_estimators=50,\n",
    "                                     #   #max_depth=5,\n",
    "                                     #   #min_child_weight=1,\n",
    "                                     #   #gamma=0,\n",
    "                                     #   #subsample=0.8,\n",
    "                                     #   #colsample_bytree=0.8,\n",
    "                                     #   #objective= 'binary:logistic',\n",
    "                                     #   #nthread=4,\n",
    "                                     #   #scale_pos_weight=1,\n",
    "                                     #  seed=1234,\n",
    "                                     #  random_state=1234),     \n",
    "                                     #'rf': ensemble.RandomForestClassifier(),\n",
    "                                     #'kersvm': svm.SVC(kernel = 'rbf', probability=True), \n",
    "                                     #'linsvm': svm.SVC(kernel = 'linear', probability=True), \n",
    "                                     ###'gpc': gaussian_process.GaussianProcessClassifier(), # doesnt exist in 0.16\n",
    "                                     #'gnb': naive_bayes.GaussianNB(),\n",
    "                                     #'gbm': ensemble.GradientBoostingClassifier(), \n",
    "                                     'logr': linear_model.LogisticRegression(),\n",
    "                                     'sgd': linear_model.SGDClassifier(),\n",
    "                                     #'mlp_nn': neural_network.MLPClassifier(shuffle=True, early_stopping=True) # doesnt exist in 0.16\n",
    "                                      #'et': ensemble.ExtraTreesClassifier(),\n",
    "                                      })\n",
    "\n",
    "clf_param_grid = {#'knn': {'n_neighbors': [1,3,5]}, #,7,9]}, #,11,13,15,17,19,21,23,25,27,29]},\n",
    "                  #'xgb1': {'max_depth': [3, 6], 'min_child_weight': [1], 'gamma': [0, 3], 'scale_pos_weight': [y_train[y_train==1].count()/y_train[y_train==0].count(), 1], 'max_delta_step': [0, 5]},\n",
    "                  #'xgb2': {'max_depth': [3, 6], 'min_child_weight': [1], 'gamma': [0, 3], 'scale_pos_weight': [y_train[y_train==1].count()/y_train[y_train==0].count(), 1], 'max_delta_step': [0, 5]},\n",
    "                  #'xgb3': {'learning_rate': [0.01, 0.025, 0.05], 'max_depth': [5,10], 'scale_pos_weight': [y_train[y_train==1].count()/y_train[y_train==0].count(), 1]},\n",
    "                  #'xgb4': {'n_estimators': [10,50,100,200], 'gamma': [0, 3], 'learning_rate': [0.01, 0.025, 0.05,0.1,0.2,0.3], 'max_depth': [5,10], 'max_delta_step': [0, 5], 'scale_pos_weight': [y_train[y_train==1].count()/y_train[y_train==0].count(), 1]},\n",
    "                  # 'rf': {'n_estimators': [50,100,200], 'max_features': [None], 'min_samples_leaf': [1, 5]}, \n",
    "                  #'kersvm': {'C': [1/32.0, 1/8.0, 1/2.0, 2, 8, 32], 'gamma':[1/32.0, 1/8.0, 1/2.0, 2, 8, 32]},\n",
    "                  #'linsvm': {'C': [1/32.0, 1/8.0, 1/2.0, 2, 8, 32]},\n",
    "                  #'gpc': {'n_restarts_optimizer': [0]},\n",
    "                  #'gnb': {},\n",
    "                  #'gbm': {'n_estimators': [100, 300, 500], 'learning_rate': [0.01, 0.1, 0.5], 'max_depth': [3,6]},\n",
    "                  'logr': {'solver': ['saga', 'sag'], 'C': [0.0001, 0.001, 0.01, 0.1, 1, 10 , 100, 1e10]},\n",
    "                  'sgd': {'loss': ['log'], 'shuffle': [True, False], 'alpha': [0.0001, 0.001, 0.01, 0.1, 1, 10 , 100, 1e10], 'penalty': ['elasticnet'], 'l1_ratio': [0,0.5,1]},\n",
    "                  #'mlp_nn': {'activation': ['relu', 'logistic'], \n",
    "                  #           'hidden_layer_sizes': [(5,), (10,), (20, ), (100,), (200,), \n",
    "                  #                                  (5,5), (10,10), (20, 20), (100,100), (200,200),\n",
    "                  #                                  (5,5,5), (10,10,10), (20, 20,20), (100,100,100), (200,200,200)], \n",
    "                  #           'alpha': [0.0001, 0.001, 0.01, .1, 1]\n",
    "                  #          } \n",
    "                  #'et': {'n_estimators': [50,100,200], 'max_features': ['auto', None], 'min_samples_leaf': [1, 5, 10]},\n",
    "                  }\n",
    "\n",
    "unique_ys = np.unique(y_train)\n",
    "sgd_lower = np.linspace(0,len(y_train),100, dtype = np.int)\n",
    "sgd_upper = sgd_lower[1:]\n",
    "sgd_lower = sgd_lower[:-1]\n",
    "\n",
    "for idx, (model_id, model_class) in enumerate(clf_models.items()):\n",
    "    \n",
    "    param_grid = clf_param_grid[model_id]\n",
    "    \n",
    "    #print \"\\n =================================== \\n\" \\\n",
    "    #      \"Starting model selection for model: {}\".format(model_id)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    if param_grid != {}:\n",
    "        combinations = build_combinations(param_grid)\n",
    "        \n",
    "    for i in range(len(combinations)):  \n",
    "    \n",
    "        comb_time = time.time()\n",
    "    \n",
    "        combination = combinations[i]\n",
    "        \n",
    "        print(model_id, i, combination)\n",
    "           \n",
    "        # load the current parameter value into the model and perform model selection\n",
    "        if combination != {}:\n",
    "            model_class.set_params(**combination)\n",
    "            \n",
    "        #print(model_class)            \n",
    "\n",
    "        if model_id in ['sgd', 'logr', 'gnb']:\n",
    "            #for batch in range(len(sgd_lower)):\n",
    "                #model_class.partial_fit(X.iloc[sgd_lower[batch]:sgd_upper[batch], :], \n",
    "                #                        y_train[sgd_lower[batch]:sgd_upper[batch]], \n",
    "                #                        classes=unique_ys)\n",
    "            #model_class.partial_fit(X.iloc[:1000, :], y_train[:1000], classes=unique_ys)\n",
    "            model_class.fit(X_train, y_train)\n",
    "            #preds = model_class.predict(X_val.iloc[:, :])\n",
    "            ##print(roc_auc_score(y_val, preds), accuracy_score(y_val, preds))\n",
    "            probs = model_class.predict_proba(X_val)[:, 1]\n",
    "            print(roc_auc_score(y_val, probs), math.sqrt(mean_squared_error(y_val, probs))) #, accuracy_score(y_val, probs))\n",
    "            \n",
    "        #np.savetxt('output/base_learners_class/r1_' + str(model_id) + '_' + str(i) + '.csv', preds, delimiter=',')\n",
    "        np.savetxt('output/base_learners_probs/r1_sams_' + str(model_id) + '_' + str(i) + '.csv', probs, delimiter=',')\n",
    "        \n",
    "        print('Time: %.5f' % (time.time()-comb_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_combinations(param_grid):\n",
    "    \n",
    "    '''\n",
    "    Create all possible hyperparameter combinations for a given model \n",
    "    '''\n",
    "  \n",
    "    combinations = []\n",
    "\n",
    "    allNames = sorted(param_grid)\n",
    "    tmp_combs = list(it.product(*(param_grid[Name] for Name in allNames)))\n",
    "    \n",
    "    for combi in tmp_combs:\n",
    "        my_dict = {}\n",
    "        my_dict[allNames[0]] = combi[0]\n",
    "    \n",
    "        for i in range(1,len(allNames)):\n",
    "            my_dict[allNames[i]] = combi[i]\n",
    "        \n",
    "        combinations.append(my_dict)\n",
    "        \n",
    "    return combinations\n",
    "\n",
    "clf_models = collections.OrderedDict({#'knn': neighbors.KNeighborsClassifier(),\n",
    "                                     #'xgb1': XGBClassifier(\n",
    "                                        #learning_rate =0.1,\n",
    "                                        #n_estimators=100,\n",
    "                                        #max_depth=5,\n",
    "                                        #min_child_weight=1,\n",
    "                                        #gamma=0,\n",
    "                                        #subsample=0.8,\n",
    "                                        #colsample_bytree=0.8,\n",
    "                                        #objective= 'binary:logistic',\n",
    "                                        #nthread=4,\n",
    "                                        #scale_pos_weight=1,\n",
    "                                        #seed=1234,\n",
    "                                        #random_state=1234),\n",
    "                                     #'xgb2': XGBClassifier(\n",
    "                                        #learning_rate =0.1,\n",
    "                                        #n_estimators=100,\n",
    "                                        #max_depth=5,\n",
    "                                        #min_child_weight=1,\n",
    "                                        #gamma=0,\n",
    "                                        #subsample=0.8,\n",
    "                                        #colsample_bytree=0.8,\n",
    "                                        #objective= 'binary:logistic',\n",
    "                                        #nthread=4,\n",
    "                                        #scale_pos_weight=1,\n",
    "                                        #seed=1234,\n",
    "                                        #random_state=1234),  \n",
    "                                     #'xgb3': XGBClassifier(\n",
    "                                        #learning_rate =0.1,\n",
    "                                     #   n_estimators=50,\n",
    "                                        #max_depth=5,\n",
    "                                        #min_child_weight=1,\n",
    "                                        #gamma=0,\n",
    "                                        #subsample=0.8,\n",
    "                                        #colsample_bytree=0.8,\n",
    "                                        #objective= 'binary:logistic',\n",
    "                                        #nthread=4,\n",
    "                                        #scale_pos_weight=1,\n",
    "                                     #   seed=1234,\n",
    "                                     #   random_state=1234),   \n",
    "                                     #'xgb4': XGBClassifier(\n",
    "                                     #   #learning_rate =0.1,\n",
    "                                     #   n_estimators=50,\n",
    "                                     #   #max_depth=5,\n",
    "                                     #   #min_child_weight=1,\n",
    "                                     #   #gamma=0,\n",
    "                                     #   #subsample=0.8,\n",
    "                                     #   #colsample_bytree=0.8,\n",
    "                                     #   #objective= 'binary:logistic',\n",
    "                                     #   #nthread=4,\n",
    "                                     #   #scale_pos_weight=1,\n",
    "                                     #  seed=1234,\n",
    "                                     #  random_state=1234),     \n",
    "                                     #'rf': ensemble.RandomForestClassifier(),\n",
    "                                     #'kersvm': svm.SVC(kernel = 'rbf', probability=True), \n",
    "                                     #'linsvm': svm.SVC(kernel = 'linear', probability=True), \n",
    "                                     ###'gpc': gaussian_process.GaussianProcessClassifier(), # doesnt exist in 0.16\n",
    "                                     #'gnb': naive_bayes.GaussianNB(),\n",
    "                                     #'gbm': ensemble.GradientBoostingClassifier(),\n",
    "                                     #'logr': linear_model.LogisticRegression(),\n",
    "                                     #'sgd': linear_model.SGDClassifier(loss='log', shuffle =False),\n",
    "                                     'mlp_nn': neural_network.MLPClassifier(shuffle=True, early_stopping=True) # doesnt exist in 0.16\n",
    "                                      #'et': ensemble.ExtraTreesClassifier(),\n",
    "                                      })\n",
    "\n",
    "clf_param_grid = {#'knn': {'n_neighbors': [1,3,5]}, #,7,9]}, #,11,13,15,17,19,21,23,25,27,29]},\n",
    "                  #'xgb1': {'max_depth': [3, 6], 'min_child_weight': [1], 'gamma': [0, 3], 'scale_pos_weight': [y_train[y_train==1].count()/y_train[y_train==0].count(), 1], 'max_delta_step': [0, 5]},\n",
    "                  #'xgb2': {'max_depth': [3, 6], 'min_child_weight': [1], 'gamma': [0, 3], 'scale_pos_weight': [y_train[y_train==1].count()/y_train[y_train==0].count(), 1], 'max_delta_step': [0, 5]},\n",
    "                  #'xgb3': {'learning_rate': [0.01, 0.025, 0.05], 'max_depth': [5,10], 'scale_pos_weight': [y_train[y_train==1].count()/y_train[y_train==0].count(), 1]},\n",
    "                  #'xgb4': {'n_estimators': [10,50,100,200], 'gamma': [0, 3], 'learning_rate': [0.01, 0.025, 0.05,0.1,0.2,0.3], 'max_depth': [5,10], 'max_delta_step': [0, 5], 'scale_pos_weight': [y_train[y_train==1].count()/y_train[y_train==0].count(), 1]},\n",
    "                  # 'rf': {'n_estimators': [50,100,200], 'max_features': [None], 'min_samples_leaf': [1, 5]}, \n",
    "                  #'kersvm': {'C': [1/32.0, 1/8.0, 1/2.0, 2, 8, 32], 'gamma':[1/32.0, 1/8.0, 1/2.0, 2, 8, 32]},\n",
    "                  #'linsvm': {'C': [1/32.0, 1/8.0, 1/2.0, 2, 8, 32]},\n",
    "                  #'gpc': {'n_restarts_optimizer': [0]},\n",
    "                  #'gnb': {},\n",
    "                  #'gbm': {'n_estimators': [100, 300, 500], 'learning_rate': [0.01, 0.1, 0.5], 'max_depth': [3,6]},\n",
    "                  #'logr': {'C': [0.0001, 0.001, 0.01, 0.1, 1, 10 , 100, 1e10]},\n",
    "                  #'sgd': {'loss': ['log', 'modified_huber'], 'alpha': [0.0001, 0.001, 0.01, 0.1, 1, 10 , 100, 1e10], 'penalty': ['elasticnet'], 'l1_ratio': [0,0.5,1]},\n",
    "                  'mlp_nn': {'activation': ['logistic', 'relu'], \n",
    "                             'hidden_layer_sizes': [(5,), (10,), (20, ), (100,), (200,), \n",
    "                                                    (5,5), (10,10), (20, 20), (100,100), (200,200),\n",
    "                                                    (5,5,5), (10,10,10), (20, 20,20), (100,100,100), (200,200,200)], \n",
    "                             'alpha': [0.0001, 0.001, 0.01, 0.1, 1]\n",
    "                            } \n",
    "                  #'et': {'n_estimators': [50,100,200], 'max_features': ['auto', None], 'min_samples_leaf': [1, 5, 10]},\n",
    "                  }\n",
    "\n",
    "unique_ys = np.unique(y_train)\n",
    "sgd_lower = np.linspace(0,len(y_train),100, dtype = np.int)\n",
    "sgd_upper = sgd_lower[1:]\n",
    "sgd_lower = sgd_lower[:-1]\n",
    "\n",
    "for idx, (model_id, model_class) in enumerate(clf_models.items()):\n",
    "    \n",
    "    param_grid = clf_param_grid[model_id]\n",
    "    \n",
    "    #print \"\\n =================================== \\n\" \\\n",
    "    #      \"Starting model selection for model: {}\".format(model_id)\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if param_grid != {}:\n",
    "        combinations = build_combinations(param_grid)\n",
    "        \n",
    "    for i in range(len(combinations)):  \n",
    "    \n",
    "        comb_time = time.time()\n",
    "        \n",
    "        combination = combinations[i]\n",
    "        \n",
    "        print(model_id, i, combination)\n",
    "    \n",
    "        # load the current parameter value into the model and perform model selection\n",
    "        if combination != {}:\n",
    "            model_class.set_params(**combination)\n",
    "            \n",
    "        if model_id == 'mlp_nn':\n",
    "            #model_class.fit(X, y_train)  \n",
    "            #for batch in range(len(sgd_lower)):\n",
    "                #model_class.partial_fit(X.iloc[sgd_lower[batch]:sgd_upper[batch], :], \n",
    "                #                        y_train[sgd_lower[batch]:sgd_upper[batch]], \n",
    "                #                        classes=unique_ys)\n",
    "            #model_class.partial_fit(X.iloc[:1000, :], y_train[:1000], classes=unique_ys)\n",
    "            model_class.fit(X_train, y_train)\n",
    "            #preds = model_class.predict(X_val.iloc[:, :])\n",
    "            ##print(roc_auc_score(y_val, preds), accuracy_score(y_val, preds))\n",
    "            probs = model_class.predict_proba(X_val)[:, 1]\n",
    "            print(roc_auc_score(y_val, probs), math.sqrt(mean_squared_error(y_val, probs))) #, accuracy_score(y_val, probs))\n",
    "            \n",
    "            test_probs = model_class.predict_proba(X_test)[:, 1]\n",
    "                        \n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "        #np.savetxt('output/base_learners_class/r2_' + str(model_id) + '_' + str(i) + '.csv', preds, delimiter=',')\n",
    "        np.savetxt('output/base_learners_probs/r2_sams_' + str(model_id) + '_' + str(i) + '.csv', probs, delimiter=',')\n",
    "        np.savetxt('output/base_learners_test_probs/r2_sams_' + str(model_id) + '_' + str(i) + '.csv', test_probs, delimiter=',')\n",
    "        \n",
    "        print('Time: %.5f' % (time.time()-comb_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
